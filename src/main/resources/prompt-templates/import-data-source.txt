# DuckDB Import Query Generator

Data Source Path:
`{{filePath}}`

Table Name:
`{{tableName}}`

Operation Mode:
`{{importMode}}`

Sample Data (5–10 lines):
```text
{{fileSample}}
```

Additional Information for LLM:
{{additionalInfo}}

---

Task:
Generate only the final DuckDB SQL that:
1. Reads the file with the correct reader (`read_csv`, `read_parquet`, `read_json`, etc.)
2. Parses timestamps and fields (`timestampformat`, `to_timestamp`, `strptime`, `regexp_extract`, `split_part`)
3. Casts types safely (`TRY_CAST`, `NULLIF`) if needed
4. Uses replace or append semantics based on Operation Mode

- replace:
  ```sql
  CREATE OR REPLACE TABLE {table_name} AS
    SELECT ... FROM read_xxx(...);
  ```
- append:
  ```sql
  CREATE TABLE IF NOT EXISTS {table_name} (...);
  INSERT INTO {table_name}
    SELECT ... FROM read_xxx(...);
  ```

_No extra commentary—SQL only._

---

## LLM Knowledge Base

CSV
```sql
read_csv(path, delim=…, header=bool, auto_detect=bool,
         columns=struct, dateformat=…, timestampformat=…)
```

Parquet
```sql
read_parquet(path)
```

JSON
```sql
read_json(path,
          format=auto|array|newline_delimited,
          columns=struct,
          ignore_errors=bool)
```

Raw/Text Logs
```sql
read_csv(path,
         delim='\n',
         header=false,
         columns={'line':'VARCHAR'},
         strict_mode=false)
```

Common Reader Options
- `auto_detect` (true/false)
- `strict_mode` (true/false)
- `compression` (auto_detect/gzip/zstd/none)
- `timestampformat` (for parsing dates)
- `columns` (to specify names/types and disable auto-detection)
- `ignore_errors` (JSON only)

---

## Examples
csv:
```sql
CREATE OR REPLACE TABLE log_entries AS
SELECT
    to_timestamp(timestamp)::TIMESTAMP AS timestamp,
    user_id,
    tenant_id,
    qualified_name
FROM read_csv('C:/logs/log.csv', header=true);
```

```sql
CREATE TABLE IF NOT EXISTS log_entries (
    timestamp TIMESTAMP,
    user_id BIGINT,
    tenant_id BIGINT,
    method_fqn VARCHAR
);

INSERT INTO log_entries
SELECT
    timestamp,
    "cmap_user-id" AS user_id,
    "cmap_developer-id" AS tenant_id,
    "cmap_message" AS method_fqn
FROM read_csv('C:/log/sample.csv',
    header=true,
    timestampformat='%b %d, %Y @ %H:%M:%S.%f');

```

json:
```sql
CREATE OR REPLACE TABLE error_log AS
SELECT
    json_extract_string(data, '$.timestamp') AS timestamp,
    json_extract_string(data, '$.level') AS level,
    json_extract_string(data, '$.service') AS service,
    json_extract_string(data, '$.env') AS environment,

    json_extract_string(data, '$.error.code') AS error_code,
    json_extract_string(data, '$.error.message') AS message,
    json_extract_string(data, '$.error.file') AS file,
    json_extract_string(data, '$.error.line') AS line,
    json_extract_string(data, '$.error.stack') AS stack_trace,

    json_extract_string(data, '$.context.user_id') AS user_id,
    json_extract_string(data, '$.context.session_id') AS session_id,
    json_extract_string(data, '$.context.ip') AS ip_address
FROM read_json('C:/logs/error_log.json') AS data;

```

semi-structured text:
```sql
CREATE OR REPLACE TABLE apache_log AS
SELECT
    split_part(t.line, ' ', 1) AS ip,
    strptime(
            split_part(split_part(t.line, '[', 2), ']', 1),
            '%d/%b/%Y:%H:%M:%S %z'
    )::TIMESTAMPTZ AS timestamp,
  split_part(split_part(t.line, '"', 2), ' ', 1) AS method,
  split_part(split_part(t.line, '"', 2), ' ', 2) AS path,
  split_part(split_part(t.line, '"', 2), ' ', 3) AS protocol,
  (regexp_extract(t.line, '" ([0-9]{3}) ', 1))::INTEGER AS status,
  NULLIF(regexp_extract(t.line, ' ([0-9]+) "[^"]*" "[^"]*"$', 1), '')::BIGINT AS bytes,
  regexp_extract(t.line, ' [0-9]+ "([^"]*)" ', 1) AS referer,
  regexp_extract(t.line, '"([^"]*)"$', 1) AS user_agent
FROM
    read_csv('C:/logs/apache_log.txt',
    auto_detect = false,
    header      = false,
    columns     = {'line':'VARCHAR'},
    delim       = '\n',
    strict_mode = false
    ) AS t
WHERE
    t.line <> '';
```
